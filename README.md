# multiFidelityNN

## Part 1: Project Description
Recently, predictive modeling has been drawing significant interest in various applications, particularly in scenarios where time-critical decisions are essential. A compelling issue in this context is to predict quantities of interest accurately when the application is intertwined with time-critical decision. Accurate prediction of specific properties require real data (a.k.a. high-fidelity data) however such physical data can be expensive, sensitive, and extremely time-consuming. A smart approach to this issue is to generate or take advantage of a giant chunk of simulated/cheap data (low-fidelity data), take a couple few of the high-fidelity data, and train the combination of both high and low-fidelity data (prevalentely known as multi-fidelity data) with a neural network. The major benefit of such multi-fidelity data aggregation using neural networks is that it significantly reduces the cost and the time of computational experiments. For this project, we propose to reproduce this idea with appropriate neural networks and examine this through multiple cases which incorporate interesting physical problems. Specifically, for this project, we will be using data generated from reaction diffusion equations (used frequently in modeling diffusion of particles or handling reactions between particles) and feed the data into our neural network system to predict the quantity of interest at any coefficient combination in a specific region accurately. 

The general process of our project proceeds as follows:

(a) Data Preparation
Prepare the data so that we have the appropriate testing, training, and validation data to feed in the implemented neural network architecture. For our application, we will (1) generate quantitative data using carefully defined mathematical equations for our network validation and (2) split our prepared data, sampled from a random 2D grid region and from (1), to testing, training, and validation data.  

(b) Design and Implement Neural Network
In this process, developing a well functioning neural network will be crucial. For our case, our network will need to capture the function/pattern between the diffusion coefficients and the quantity of interest. Accordingly, convolutional neural networks are expected to be used with additional graph or linear networks if necessary to achieve such goal.

(c) Validate Neural Network Architecture with Simple Example Case
Prior to applying the neural network to the main dataset, we examine our architecture with a simple example where low and high-fidelity are generated by carefully created mathematical functions. Once the calculated error measure between the predicted and the real data (high-fidelity model) is determined to be acceptable, we proceed to our final step.

(d) Application (Reaction Diffusion Equation)
Finally, we feed our neural network architecture our prepared data and measure the MSE of: (1) low-fidelity function, (2) high-fidelity function, (3) the real data (high-fidelity model) against the validation data set.

In summary, this project involves a systematic approach to leverage multi-fidelity data aggregation using neural networks for predicting quantities of interest; specifically, data generated from reaction diffusion equations. The application of this methodology to reaction-diffusion equations provides a practical and illustrative context for exploring the capabilities and benefits of this approach in real-world scenarios such as in understanding spatial patterns in dynamic systems utilizing a combination of mathematical framework and a fine neural network architecture.

## Part 2: Datasets

source (download link and associated paper(s) offering the dataset(s))
differences between the train and validation subsets, which you think are important from your project point of view
number of distinct objects/subjects represented in the data, number of samples per object/subject (if applies)
brief characterization of samples: resolution, sensors used, ambient conditions, sampling frequency for audio, etc. (whichever applies)
attach a few data samples (if possible) to your report to illustrate what type of data goes to each subset.


For our simple example case, explained as above from step (c), we approximate a one-dimensional function based on data from high and low fidelities. 
Each level of fidelities are generated from the following functions:
$y_L(x) = A(6x-2)^2sin(12x-4) + B(x-0.5) + C, x \in [0,1]$
$y_H(x) = (6x-2)^2sin(12x-4)$
where y_L(x) generates low-fidelity data, and y_H(x) generates high-fidelity data