# multiFidelityNN

## Part 1: Project Description
Recently, predictive modeling has been drawing significant interest in various applications, particularly in scenarios where time-critical decisions are essential. A compelling issue in this context is to predict quantities of interest accurately when the application is intertwined with time-critical decision. Accurate prediction of specific properties require real data (a.k.a. high-fidelity data) however such physical data can be expensive, sensitive, and extremely time-consuming. A smart approach to this issue is to generate or take advantage of a giant chunk of simulated/cheap data (low-fidelity data), take a couple few of the high-fidelity data, and train the combination of both high and low-fidelity data (prevalentely known as multi-fidelity data) with a neural network. The major benefit of such multi-fidelity data aggregation using neural networks is that it significantly reduces the cost and the time of computational experiments. For this project, we propose to reproduce this idea with appropriate neural networks and examine this through multiple cases which incorporate interesting physical problems. Specifically, for this project, we will be using data generated from reaction diffusion equations (used frequently in modeling diffusion of particles or handling reactions between particles) and feed the data into our neural network system to predict the quantity of interest at any coefficient combination in a specific region accurately. 

The general process of our project proceeds as follows:

(a) Data Preparation
Prepare the data so that we have the appropriate testing, training, and validation data to feed in the implemented neural network architecture. For our application, we will (1) generate quantitative data using carefully defined mathematical equations for our network validation and (2) split our prepared data, sampled from a random 2D grid region and from (1), to testing, training, and validation data.  

(b) Design and Implement Neural Network
In this process, developing a well functioning neural network will be crucial. For our case, our network will need to capture the function/pattern between the diffusion coefficients and the quantity of interest. Accordingly, convolutional neural networks are expected to be used with additional graph or linear networks if necessary to achieve such goal.

(c) Validate Neural Network Architecture with Simple Example Case
Prior to applying the neural network to the main dataset, we examine our architecture with a simple example where low and high-fidelity are generated by carefully created mathematical functions. Once the calculated error measure between the predicted and the real data (high-fidelity model) is determined to be acceptable, we proceed to our final step.

(d) Application (Reaction Diffusion Equation)
Finally, we feed our neural network architecture our prepared data and measure the MSE of: (1) low-fidelity function, (2) high-fidelity function, (3) the real data (high-fidelity model) against the validation data set.

In summary, this project involves a systematic approach to leverage multi-fidelity data aggregation using neural networks for predicting quantities of interest; specifically, data generated from reaction diffusion equations. The application of this methodology to reaction-diffusion equations provides a practical and illustrative context for exploring the capabilities and benefits of this approach in real-world scenarios such as in understanding spatial patterns in dynamic systems utilizing a combination of mathematical framework and a fine neural network architecture.

## Part 2: Datasets

For our simple example case [1], explained as above from step (c), we approximate a one-dimensional function based on data from high and low fidelities where function is continuous and both level of fidelity generator functions are linearly correlated. 
Each level of fidelities are generated from the following functions: <br>
$y_L(x) = A(6x-2)^2sin(12x-4) + B(x-0.5) + C, x \in [0,1]$ <br>
$y_H(x) = (6x-2)^2sin(12x-4)$ <br>
where y_L(x) generates low-fidelity data, and y_H(x) generates high-fidelity data (true function). In addition, we let A = 0.5, B = 10, and C = -5. Note that the training data at the low- and high-fidelity level are respectively $x_{L}$ = {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1} and $x_{H}$ = {0, 0.4, 0.6, 1}. <br>
We test our neural network's functional by first using only the high-fidelity data generated from $y_H(x)$ and comparing it with the one feeding low-fidelity data generated from $y_L(x)$ to ensure that our neural network successfully predicts the true function when multi-fidelity data are fed. Note that when only using high-fidelity data, we only will need to keeps the layers of the network where high-fidelity data are processed. 

In addition, we test if the model can capture complex nonlinear correlations between low and high fidelity generator functions. The corresponding functions are the following:<br>
$y_L(x) = sin(8\pi x), x \in [0,1]$ <br>
$y_H(x) = (x-\sqrt{2})y_L^2$ <br>
where we plan to employ uniformly distributed 51 points to generate low fidelity data and 12 data points to generate high fidelity data as the training data.

Note that if our neural network successfully works on the two simple example cases in a short amount of time, we plan to test functionality through inverse PDE problems with nonlinearities (IF we are ahead of time). 

Once we finish confirming the functional of our neural network, we proceed to 
our reaction diffusion application problem to derive and predict the solution of the reaction-diffusion PDE. In this case, we consider the 2D reaction-diffusion equation satisfying a PDE in the form as the following: <br>
$\partial_t u = D_u \partial_{xx}u + D_u \partial_{yy}u + R_u$ <br>
$\partial_t v = D_v \partial_{xx}v + D_v \partial_{yy}v + R_v$ <br>
where $u = u(t,x,y)$ is the activator and $v = v(t,x,y)$ is the inhibitor. Note that activator and inhibitor is simply two types of chemical components involved in our system which is generally used to explain pattern formation dynamics or various spatial structures. To be more detailed, activator promotes production of others and itself so it is likely to have positive influence on reaction terms where on the other hand, invibitors suppresses production of itself and others so is likely to have negative influence on the reaction terms. So, the interaction between the activator and inhibitor stimulates unique spatial patterns and structures. Accordingly, $R_u = R_u(u,v)$ and $R_v = R_v(u,v)$ are respectively the activator and inhibitor reaction function. <br>
Furthermore, for the boundary conditions, we consider a no-flow Neumann boundary condition (i.e. $D_u \partial_{x}u = 0, D_v \partial_{x}v = 0, D_u \partial_{y}u = 0, D_v \partial_{y}v = 0$) where Neumann boundary condition is simply a certain type of boundary condition used in PDEs and by no-flow, this generally infers that there is no net flow across the boundary (i.e. zero net movement of the substance across the boundary). <br>
Also, we consider uncertain initial condition $u(0,x,y),v(0,x,y) ~ N(0,1) \forall x,y$ where $D_u$ and $D_v$ are the corresponding diffusion coefficients. <br>
For our reaction functions, Fitzhugh-Nagoumo equations are used: <br>
$R_u(u,v) = u - u^3 - k - v$ <br>
$R_v(u,v) = u - v$ <br>
where $k = 5*10^{-3}$. <br>
This particular set of equations exhibit interaction of variables with different excitement levels and can express and display the transition between resting and excited states under certain conditions.

Specifically, we are interest in predicting a scalar value of the mean concentration as a function of the diffusion coefficients $D_{u}$ and $D_{v}$ where the scalar value represents the mean concentration of our inhibitor u in the region [0,1]x[-1,0] at t = 1. 

With respect to the problem set, we generate two predictors using functionalities from the PDEBench library[2], where the modified code is attached to this repository, which are each the high-fidelity predictor using a discretized domain of size 128x128 and the low-fidelity predictor using a 64x64 spatial mesh.

Note that there might be slight modification of the specific domain size due to computational cost throughout the project application.

As for the test, training, and validation data sets, we randomly split the generated data as the generated data is just simply simulated data. Specifically, the current test and training data set on the repository is split by 50:50 of the sampled data. Including the validation data set, we start with initial split ratio: (60:20:20 = training:validation:testing) and make modifications as we proceed with the project.

[Source]

[1] arXiv:1903.00104 [physics.comp-ph]

[2] https://github.com/pdebench/PDEBench


## Part 3: First solution 

Our first solution of the neural network project includes the implementation of the simple example case [1] explained from part 2. To reiterate, in our approach, we aim to construct an approximation of a one-dimensional function that exhibits continuity, leveraging datasets characterized by both high and low fidelity levels. The underlying principle guiding this approximation is predicated on the observation that the generators of both fidelity levels share a linear correlation, allowing for a more nuanced understanding and modeling of the function based on data spanning the spectrum from low to high fidelity.

(1) Justification of neural network architecture
The neural network architecture we have developed incorporates several Multi-Layer Perceptron (MLP) neural networks, each functioning in concert within the overarching framework. This composition enables our architecture to leverage the distinct advantages and capabilities of multiple MLPs, working synergistically to achieve more complex and nuanced data processing and analysis tasks than would be possible with a single MLP network. Specifically, we plan to incorporate three MLP neural networks where each MLP neural network serves for different purposes.
The first MLP neural network accepts low fidelity data and trains the low fidelity input data to predict the corresponding y value. Recall that in the beginning of this documentation, we articulated that we assume we have a bunch of low fidelity data and very few high fidelity data due to reduce high computational cost and time. Hence, the first MLP neural network is crucial since we want to ensure that we have a good output using the low fidelity input data before we feed the output to the network which trains the output and the high fidelity input data in order to yield our target result.

The details of our first MLP neural network is as follows:
Number and type of layers: we use [64,64] for our layers and for the activation function we use tanh. 

Weight: we use Xavier normal for the weight and bias when initializing our layers. 

Loss function: we use MSE for the loss function where we square the absolute difference between the output y and the actual y value and take the mean

Optimization algorithm: by comparing Adam and stochastic gradient descent (SGD) optimization algorithms, we confirmed that SGD outputs better accuracy so we pick SGD for the optimization algorithm

Following this, the architecture introduces the second and third MLP neural networks, both of which are fed high fidelity input data alongside the output derived from the first MLP neural network. These networks are distinguished by their activation functions: the second employs a linear activation function, while the third opts for a non-linear activation function. This bifurcation is deliberate, designed to capture the potential linear or non-linear relationships that may exist between the low and high fidelity data sets. Initially, our strategy focuses on implementing and validating the functionality of two distinct MLP neural networks—specifically, the first network (dedicated to processing low fidelity data) and the third network (targeted at non-linear correlations with high fidelity data). This phased approach allows us to ensure that each component operates effectively before we proceed to integrate the entire proposed architecture.

The details of our third MLP neural network is as follows:

Number and type of layers: we use [64,64] for our layers and for the activation function we use tanh. 

Weight: we use Xavier normal for the weight and bias when initializing our layers. 

Loss function: we use MSE for the loss function where we square the absolute difference between the output y and the actual y value and take the mean

Optimization algorithm: by comparing Adam and stochastic gradient descent (SGD) optimization algorithms, we confirmed that SGD outputs better accuracy so we pick SGD for the optimization algorithm

Note that each level of fidelities are generated from the following functions: <br>
$y_L(x) = A(6x-2)^2sin(12x-4) + B(x-0.5) + C, x \in [0,1]$ <br>
$y_H(x) = (6x-2)^2sin(12x-4)$ <br>
where y_L(x) generates low-fidelity data, and y_H(x) generates high-fidelity data (true function). In addition, we let A = 0.5, B = 10, and C = -5. Note that the training data at the low- and high-fidelity level are respectively $x_{L}$ = {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1} and $x_{H}$ = {0, 0.4, 0.6, 1}. <br>

(2) Observed accuracy of neural network architecture

Running throughout epochs, notice that the loss value significantly decreases as the epoch increase.
However, as we were trying to test with the validation and test data, the code fails. This is an issue we need to work on which is also re-addressed in the next paragraph.


(3) Commentary related to the observed accuracy and ideas for improvements (to be implemented in the final solution)

There are a couple things that needs to be improved which are listed above:

To move closer to our ultimate goal, it's essential to add the second neural network to our setup. This step is crucial to ensure our neural network architecture well captures the linear or non-linear relationship that exist between the low fidelity and the high fidelity equation. 

In addition, up until now, we've been stitching two networks together with separate pieces of code, but this isn't the most efficient approach. It's also not easy to read.
What we need is a smoother way to combine all three MLP (Multi-Layer Perceptron) neural networks into a single, streamlined model. One good solution might be to design a separate class that can handle this task, making the whole system work as one unit. This is also crucial which is likely to resolve the issue where we failed to use our model to test with the validation and test data. By combining the MLP networks together to a singular model, we hope to efficiently test and use our model.

Besides refining our model's structure, we also need to focus on how well it's performing. To do this, we'll look at the residual error values, which tell us how far off our predictions are from the actual values. But numbers alone might not give us the full picture. A great way to see our network's accuracy in action is by plotting both the actual values and our predicted values on a graph. This visual comparison can quickly show us how close our predictions are to reality, helping us understand our model's effectiveness at a glance.

## Part 4: Final Report

Code Instruction: run the file **run_exp.py** by running command line: `python3.10 run_exp.py` or `python3 run_exp.py`. this runs the simple example though our final deep multli-fidelity neural network model and generates training loss, validation accuracy, test accuracy and a plot with both high and low fidelity lines as well as the prediction line produced by our multi-fidelity network. Note that in our `main` branch we include the completely necessary files for running the `run_exp.py` file.

Database Collection Description: In the implementation of our simple examples, we utilized mathematical equations to systematically generate both high and low fidelity data, affording us considerable flexibility in determining the size of our database to tailor it specifically to the needs of our experimental design. The code generating the data was initially constructed to generate .txt files but then was modifed so that it would work accordingly with the main runner file, with both types of fidelity data derived from predetermined mathematical models, for simplicity. 
Since our data set is simply generated with random sequence of values that are within the defined range, this allow us to control the detail of the data points. To mitigate potential biases and ensure the generalizability of the model, the data sets were divided into training, validation, and testing subsets, split randomly yet in manually specified proportions; it is critical to emphasize that the differences between these subsets should not be substantial, as dramatic variations could potentially skew the performance metrics and lead to misleading conclusions about the model's efficacy. Hence, the main difference between the training and validation subset is simply the size as we choose to make the training dataset to be the largest portion of the entire generated data. The key principle guiding the partitioning process was to guarantee a uniform distribution of data points across all subsets, ensuring that our neural network architecture is exposed to, and trained on, a comprehensive range of values within the specified bounds, promoting the robustness of the model and enabling it to perform reliably across diverse scenarios and maximize its predictive accuracy. This balanced approach is crucial for developing reliable predictive models capable of operating effectively in time-critical applications. Note that for the `run_exp.py` file, we use the first example where the high fidelity and the low fidelity data generating functions are linearly correlated.
The corresponding functions are: <br>
(1) low-fidelity data generator: $y_L(x) = A(6x-2)^2sin(12x-4) + B(x-0.5) + C, x \in [0,1]$ <br>
(2) high-fidelity data generator: $y_H(x) = (6x-2)^2sin(12x-4)$ <br>

Note that other example cases are hidden in order to prevent overwriting other codes. <br>

Classification Accuracy Achieved on the Test Set: For the clasification accuracy, we set up the evaluation criteria such that when the average of |prediction - actual| for low and high fidelity is less than or equal to 5 (i.e. (|prediction - actual| + |prediction - actual|) / 2) <= threshold (= 5), we consider the prediction to be correct. 
 Initially, our test accuracy was under 0.25, which was not satisfactory. Also, our training loss was extremely large. In an attempt to fix such issue of error occurance, we tried implementing a couple of methods to fix such error.
We first tried modifying the loss function calculation which involved two major changes. Initially, we added a regularization factor which consists of a constant $\lambda$ multiplied by the sum of biases used from one of the high-fidelity neural network (in our particular case, the bias of the high-fidelity neural network that captures the linear correlation was used). In addition, instead of using a single mean squared error (MSE) that only incorporated the high fidelity parameters, we also included a MSE incorporating the low fidelity parameters. <br>
We also increased the data size and generated 10 times more data for all training, testing, and validation subsets. <br>
These implementations and modifications drastically brought up the test accuracy in which the accuracy range is: [0.80, 0.95] even with small epochs (<=100).
However, not that the challenge is when our threshold drops lower than 5, then the test accuracy starts dropping drastically. This issue becomes particularly concerning in more complex and sensitive scenarios, such as our initial tests with physical diffusion-reaction data, where the accuracy was nearly zero due to the relatively loose threshold setting. Moving forward, our primary objective is to refine and optimize our multi-fidelity neural network to handle complex, high-dimensional physical problems like diffusion-reaction, ensuring robust and accurate predictions even under stringent threshold conditions. This will involve not only further tuning of the network's architecture and training procedures but also a more critical examination of the interaction between low and high fidelity data within the model to foster deeper learning and better generalization across diverse data sets.